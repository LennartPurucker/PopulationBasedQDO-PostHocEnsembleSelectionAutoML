# Assembled-Ask: Build Metatasks from the Search Results of Auto-Sklearn

## Installation

To run this part of our code, you will require a Linux environment (due to Auto-Sklearn).
We suggest to use [Docker](https://www.docker.com/) or [Singularity](https://sylabs.io/docs/).
Once you have set up one of the above, use the dockerfile or singularity def file in the `environment` directory to
build the python environment required to run the following scripts.

## Advanced Details

The files `../benchmark/setup_data/bacc_benchmark_data.json`, `../benchmark/setup_data/roc_benchmark_data.json` contain
the exact parameters we have supplied to the scripts below.
As reproducing the exact data requires a lot of time (4 hours per fold per dataset) and a lot of memory
(32-126 GB per fold per dataset), we provide a minimal example that shows that our code works.

We limited the number of CPUs outside the scripts using [SLURM](https://slurm.schedmd.com/documentation.html).
You could also limit the CPUs with Linux, Docker, or Singularity.
For the minimal example we ignore this and default to using all available CPUs.

## Minimal Example

Thought the minimal example we will use "-1" as an OpenML task ID. This will adjust the `time_limit` to be in minutes
instead of hours and use a small toy dataset from sklearn.

### 1. Run Auto-Sklearn on the dataset

First run the following command in the CLI while being in the docker/singularity environment or start the scripts with
an IDE and the container as interpreter.
This wil run Auto-Sklearn on each fold of the toy dataset for 1 minute wit 1 GB of RAM and all available CPU cores.
Thereby, Auto-Sklearn optimizes for the metric balanced accuracy.
The output is saved under `output/benchmark/output/minimal_example`.

```shell
python run_ask_on_metatask.py -1 1 1 0,1,2,3,4,5,6,7,8,9 balanced_accuracy minimal_example
```

### 2. Collect the Prediction Data Generated by Auto-Sklearn

Next, we run the following command to parse and collect the prediction data generated by Auto-Sklearn in a format
such that we can re-use it in the next step.

```shell
python run_collect_predictor_data.py -1 0,1,2,3,4,5,6,7,8,9 balanced_accuracy minimal_example norefit save_space
```

### 3. Save and Prune the Prediction Data

Finally, we build the final data structure (a Metatask from the Assembled Framework) used for evaluating the ensemble
methods. We have to run this twice for each pruning method.

```shell
python run_full_metatask_builder.py -1 1 1 balanced_accuracy csv minimal_example TopN
python run_full_metatask_builder.py -1 1 1 balanced_accuracy csv minimal_example SiloTopN
```

After running all scripts, the final data structures for each pruning method is going be stored under
`benchmark/output/minimal_example/{{task_id}}/final_ouput`.

## Detail Usage Documentation

To build a metatask with auto-sklearn, you have to execute the following scripts in order.
You have to do this for each metric and all the details in the metric specific files (MSF)
(e.g., `../benchmark/setup_data/bacc_benchmark_data.json`, `../benchmark/setup_data/roc_benchmark_data.json`)
as noted below to fully reproduce all the data for all base models.

1) `python run_ask_on_metatask.py task_id time_limit memory_limit folds_to_run metric_name base_folder_name`
    * `task_id`: an OpenML task ID (for testing, pass `-1`)
        * see `../benchmark/setup_data/task_data.json` for all used task IDs.
    * `time_limit`: the time limit / search time for auto-sklearn in hours
        * see MSF: "framework_time_hours"
    * `memory_limit`: the memory limit for the search of auto-sklearn in GB
        * see MSF: "framework_memory_gbs" and add "special_cases/framework_memory_gbs" for the listed Task IDs
    * `folds_to_run`: the folds to run (for parallelization). Either a list of a subset of folds or all folds,
      e.g, `0,1,2,3,4,5,6,7,8,9`. Alternatively, only one fold, e.g., `0`.
    * `metric_name`: metric name of the metric to be optimized by ask, we expect the import name of the metric
        * see MSF: "framework_extras/metric"
    * `base_folder_name`:  the name of the folder where the results are supposed to be stored. (
      Under `benchmark/output/base_folder_name`)
2) `python run_collect_predictor_data.py task_id folds_to_run metric_name base_folder_name refit save_disc_space`
    * `task_id`: an OpenML task ID (for testing, pass `-1`)
    * `folds_to_run`: the folds to run (for parallelization). Either a list of a subset of folds or all folds,
      e.g, `0,1,2,3,4,5,6,7,8,9`. Alternatively, only one fold, e.g., `0`.
    * `metric_name`: metric name of the metric to be optimized by ask, we expect the import name of the metric
      * see MSF: "framework_extras/metric"
    * `base_folder_name`: same as for previous step, the name of the folder where the results are supposed to be stored.
    * `refit`: If "norefit" no refitting is happening, if "refit" refitting is happening.
        * see MSF: "framework_extras/refit"; refit is not fully supported at the moment and potential future work.
    * `save_disc_space`: if "save_space" all no longer needed ask data will be deleted. Otherwise, nothing happens which
      results in much larger disk space requirements.
        * see MSF: "framework_extras/save_disc_space"
3) `python run_full_metatask_builder.py task_id time_limit memory_limit metric_name use_hdf base_folder_name filter_predictors`
    * `task_id`: an OpenML task ID (for testing, pass `-1`)
    * `time_limit`: the time limit / search time for auto-sklearn in hours (need for metadata)
        * see MSF: "framework_time_hours"
    * `memory_limit`: the memory limit for the search of auto-sklearn in GB (need for metadata)
        * see MSF: "framework_memory_gbs" and add "special_cases/framework_memory_gbs" for the listed Task IDs
    * `metric_name`: metric name of the metric to be optimized by ask, we expect the import name of the metric
        * see MSF: "framework_extras/metric"
    * `file_format`: Allowed formats are {"csv", "hdf"}. Default is .csv.
        * see MSF: "framework_extras/file_format" for the default which overruled by "special_cases/file_format" for the
          listed Task IDs
    * `base_folder_name`: same as for previous step, the name of the folder where the results are supposed to be stored.
    * `filter_predictors`: Either "TopN" or "SiloTopN". Decides how the predictors are pruned.

### Details

* We default to n_jobs=-1. Moreover, the memory passed to the script is split between all jobs like it was done in the
  AutoML Benchmark.
* If you parallelize: You have to first run `run_ask_on_metatask.py` for all folds; then
  run `run_collect_predictor_data.py` for all folds
  and finally can use `run_full_metatask_builder.py`.

### Known Problems

* For some datasets (specifically the OpenML Task 359959), Auto-Sklearn produces an SGD or Passive Aggressive Model that
  returns nan values
  for the prediction probabilistic during predict_proba for the test data on a few instances. We fix this by adding a
  prediction probability of 1 for the predicted class.